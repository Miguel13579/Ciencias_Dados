{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miguel13579/Ciencias_Dados/blob/main/Trabalhofinal1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit faiss-cpu sentence-transformers transformers accelerate\n",
        "!wget -q -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "!tar -xvzf ngrok-v3-stable-linux-amd64.tgz\n",
        "!mv ngrok /usr/local/bin\n",
        "#Instala dependências para NLP, RAG e interface.\n",
        "#Baixa e instala o ngrok para expor a interface Streamlit na web."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gplyQ-rjd-Ex",
        "outputId": "533108a9-4b7e-4634-d5a5-83a4689e8719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hngrok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj7YPP7WaB-C",
        "outputId": "7becf318-7b3a-4759-d309-b8c9b76e2098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2025.4.26)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.20.0)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.81.0\n",
            "    Uninstalling openai-1.81.0:\n",
            "      Successfully uninstalled openai-1.81.0\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZogGHaUJV_Hk",
        "outputId": "a12c3772-1041-45ae-86cb-b6e9414abd81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5WR6eKsOYXK",
        "outputId": "628f670d-c04f-4b4b-a222-2f2c28119d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"Perguntas_IA_Sistema_RAG.xlsx\")\n",
        "documentos = df[\"Pergunta\"].dropna().tolist()\n",
        "print(f\"Carregados {len(documentos)} documentos para indexar.\")\n",
        "#Carrega o conteúdo da planilha como base documental."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_uJ311LZIzL",
        "outputId": "cbe8404b-902f-4c2d-852f-787ac0d75a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregados 10 documentos para indexar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "gerador = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\", max_new_tokens=300)\n",
        "\n",
        "def buscar_similares(pergunta, k=3):\n",
        "    embed = modelo_embedding.encode([pergunta], convert_to_numpy=True).astype(\"float32\")\n",
        "    dists, indices = index.search(embed, k)\n",
        "    return [(textos[i], dists[0][j]) for j, i in enumerate(indices[0])]\n",
        "\n",
        "def gerar_resposta_rag(pergunta, k=3):\n",
        "    trechos = buscar_similares(pergunta, k)\n",
        "\n",
        "    # Elimina duplicatas, mantém apenas os textos\n",
        "    textos_unicos = list(dict.fromkeys([trecho[0] for trecho in trechos]))\n",
        "\n",
        "    contexto = \"\\n\".join(textos_unicos)\n",
        "    prompt = f\"\"\"### Instrução:\n",
        "Com base nos textos abaixo, responda à pergunta do usuário.\n",
        "\n",
        "### Contexto:\n",
        "{contexto}\n",
        "\n",
        "### Pergunta:\n",
        "{pergunta}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "\n",
        "    resposta = gerador(prompt)[0][\"generated_text\"].split(\"### Resposta:\")[-1].strip()\n",
        "    return resposta, textos_unicos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfopxvV2cPxq",
        "outputId": "d75861cf-0695-4fb8-cdda-ca9bcfa88a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # leve e eficiente\n",
        "embeddings = model.encode(documentos, convert_to_numpy=True)\n",
        "print(f\"Embeddings gerados: shape {embeddings.shape}\")\n",
        "#Gera embeddings vetoriais dos documentos.\n",
        "#all-MiniLM-L6-v2 é leve e eficiente."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MuCKm_NZTTT",
        "outputId": "2f2a1ae1-edc9-4d2d-d3dc-6aba87144127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings gerados: shape (10, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "print(f\"Índice FAISS criado com {index.ntotal} vetores.\")\n",
        "#Cria um índice vetorial simples (sem compressão, flat).\n",
        "#dim é a dimensão dos embeddings (normalmente 384 para MiniLM)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zaV63O0ZbdD",
        "outputId": "3ce1a332-0fc3-4ebf-e8a4-87e165c47994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice FAISS criado com 10 vetores.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def buscar_similares(query, k=3):\n",
        "    emb_query = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(emb_query, k)\n",
        "    return [(documentos[i], D[0][idx]) for idx, i in enumerate(I[0])]\n",
        "#Retorna os k documentos mais similares a uma pergunta."
      ],
      "metadata": {
        "id": "e1vxDP9vZeTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"O que é Inteligência Artificial e como ela funciona?\"\n",
        "resposta, trechos = gerar_resposta_rag(pergunta)\n",
        "\n",
        "print(\"Pergunta:\", pergunta)\n",
        "print(\"Resposta:\", resposta)\n",
        "print(\"\\nTrechos usados:\")\n",
        "for trecho in trechos:\n",
        "    print(\"-\", trecho)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmn3RYBkcw3C",
        "outputId": "79352eaa-3595-48f9-c7b9-7ba862455949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta: O que é Inteligência Artificial e como ela funciona?\n",
            "Resposta: Avaliar a performance de um modelo de IA é complicado, mas pode ser feito com o uso de métodos de avaliação. No entanto, existem muitos métodos que podem ser usados para avaliar a performance de um modelo de IA, incluindo:\n",
            "- Métricas de perda\n",
            "- Métricas de confian\n",
            "\n",
            "Trechos usados:\n",
            "- O que é Inteligência Artificial e como ela funciona?\n",
            "- Como avaliar a performance de um modelo de IA?\n",
            "- O que é um sistema RAG e como ele melhora a geração de respostas?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "\n",
        "st.title(\"Sistema RAG - Perguntas e Respostas\")\n",
        "\n",
        "pergunta = st.text_input(\"Digite sua pergunta:\")\n",
        "\n",
        "if pergunta:\n",
        "    resposta = gerar_resposta_rag(pergunta)\n",
        "    st.markdown(\"### Resposta:\")\n",
        "    st.write(resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWwZxcmUZxXr",
        "outputId": "8d000b5f-5699-4f48-a762-0627b12db5c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-31 00:12:29.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.710 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.713 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:29.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "# ---------------------------\n",
        "# Carregar modelo de embeddings e LLM\n",
        "# ---------------------------\n",
        "modelo_embedding = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Leve, ideal para Colab\n",
        "gerador = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\", max_new_tokens=300)\n",
        "\n",
        "# ---------------------------\n",
        "# Simular base vetorial FAISS\n",
        "# ---------------------------\n",
        "# Suponha que esses são os textos indexados do seu .xlsx\n",
        "textos = [\n",
        "    \"Inteligência Artificial é a capacidade de máquinas realizarem tarefas humanas.\",\n",
        "    \"A IA funciona por meio de algoritmos que aprendem com dados.\",\n",
        "    \"Redes neurais artificiais são inspiradas no cérebro humano.\"\n",
        "]\n",
        "\n",
        "# Gerar embeddings\n",
        "embeddings = modelo_embedding.encode(textos, convert_to_numpy=True).astype(\"float32\")\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "\n",
        "# ---------------------------\n",
        "# Função de busca no índice + geração de resposta\n",
        "# ---------------------------\n",
        "def buscar_similares(pergunta, k=3):\n",
        "    embed = modelo_embedding.encode([pergunta], convert_to_numpy=True).astype(\"float32\")\n",
        "    dists, indices = index.search(embed, k)\n",
        "    return [textos[i] for i in indices[0]]\n",
        "#Interface amigável onde o usuário insere perguntas.\n",
        "#Usa st.write() para exibir a resposta e os trechos base.\n",
        "\n",
        "def gerar_resposta_rag(pergunta, k=3):\n",
        "    trechos = buscar_similares(pergunta, k)\n",
        "    contexto = \"\\n\".join(trechos)\n",
        "    prompt = f\"\"\"### Instrução:\n",
        "Com base nos textos abaixo, responda à pergunta do usuário.\n",
        "\n",
        "### Contexto:\n",
        "{contexto}\n",
        "\n",
        "### Pergunta:\n",
        "{pergunta}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "\n",
        "    resposta = gerador(prompt)[0][\"generated_text\"].split(\"### Resposta:\")[-1].strip()\n",
        "    return resposta, trechos\n",
        "\n",
        "# ---------------------------\n",
        "# Interface Streamlit\n",
        "# ---------------------------\n",
        "st.set_page_config(page_title=\"Sistema RAG\", layout=\"centered\")\n",
        "st.title(\"🤖 Sistema RAG com TinyLlama + FAISS\")\n",
        "\n",
        "pergunta = st.text_input(\"Digite sua pergunta:\")\n",
        "if st.button(\"Responder\") and pergunta:\n",
        "    with st.spinner(\"Gerando resposta...\"):\n",
        "        resposta, trechos = gerar_resposta_rag(pergunta)\n",
        "    st.subheader(\"📌 Resposta:\")\n",
        "    st.write(resposta)\n",
        "    st.subheader(\"📚 Trechos usados:\")\n",
        "    for t in trechos:\n",
        "        st.markdown(f\"- {t}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-MV3I1Rd3FC",
        "outputId": "dc639b8d-fe8e-4e73-b4dc-d70a6b0a5389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
            "Device set to use cpu\n",
            "2025-05-31 00:12:46.068 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.069 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.071 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.072 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.073 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.074 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.075 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.077 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.078 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.081 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-05-31 00:12:46.083 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken SEU_TOKEN_NGROK\n",
        "#Permite expor o app Streamlit fora do Colab."
      ],
      "metadata": {
        "id": "tx5Zop5seBgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0df5968-a88c-4c74-81f0-249c5c1e54f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "modelo_embedding = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "gerador = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", device_map=\"auto\", max_new_tokens=300)\n",
        "\n",
        "textos = [\n",
        "    \"Inteligência Artificial é a capacidade de máquinas realizarem tarefas humanas.\",\n",
        "    \"A IA funciona por meio de algoritmos que aprendem com dados.\",\n",
        "    \"Redes neurais artificiais são inspiradas no cérebro humano.\"\n",
        "]\n",
        "\n",
        "embeddings = modelo_embedding.encode(textos, convert_to_numpy=True).astype(\"float32\")\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(embeddings)\n",
        "\n",
        "def buscar_similares(pergunta, k=3):\n",
        "    embed = modelo_embedding.encode([pergunta], convert_to_numpy=True).astype(\"float32\")\n",
        "    dists, indices = index.search(embed, k)\n",
        "    return [textos[i] for i in indices[0]]\n",
        "\n",
        "def gerar_resposta_rag(pergunta, k=3):\n",
        "    trechos = buscar_similares(pergunta, k)\n",
        "    contexto = \"\\n\".join(trechos)\n",
        "    prompt = f\"\"\"### Instrução:\n",
        "Com base nos textos abaixo, responda à pergunta do usuário.\n",
        "\n",
        "### Contexto:\n",
        "{contexto}\n",
        "\n",
        "### Pergunta:\n",
        "{pergunta}\n",
        "\n",
        "### Resposta:\"\"\"\n",
        "\n",
        "    resposta = gerador(prompt)[0][\"generated_text\"].split(\"### Resposta:\")[-1].strip()\n",
        "    return resposta, trechos\n",
        "\n",
        "st.set_page_config(page_title=\"Sistema RAG\", layout=\"centered\")\n",
        "st.title(\"🤖 Sistema RAG com TinyLlama + FAISS\")\n",
        "\n",
        "pergunta = st.text_input(\"Digite sua pergunta:\")\n",
        "if st.button(\"Responder\") and pergunta:\n",
        "    with st.spinner(\"Gerando resposta...\"):\n",
        "        resposta, trechos = gerar_resposta_rag(pergunta)\n",
        "    st.subheader(\"📌 Resposta:\")\n",
        "    st.write(resposta)\n",
        "    st.subheader(\"📚 Trechos usados:\")\n",
        "    for t in trechos:\n",
        "        st.markdown(f\"- {t}\")\n",
        "#Junta tudo em um único arquivo executável com streamlit run app.py.\n",
        "#Útil para organização e reusabilidade."
      ],
      "metadata": {
        "id": "Ac_Cq94aeCQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f82e38-6f3a-4c33-84d4-de1a0178362d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    }
  ]
}